{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize api instance\n",
    "twitter_api = twitter.Api(consumer_key='8sKFgj4r3aCfmvHLNvKTLuS2A',\n",
    "                        consumer_secret='qqeo83EDXzX3YqMc8wC5HzeSxDn7nJoOJpP11ExIa8Ediyj3RC',\n",
    "                        access_token_key='4276054402-ghsIYrOMEVHuApV5XHtR4qEQidNGIa50aQAhvLp',\n",
    "                        access_token_secret='waTDjk0TkM66Nq32rGpd4v2zdJRf6TaMAHfNpaPxBsqUl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Wed Nov 25 16:03:12 +0000 2015\", \"default_profile\": true, \"description\": \"LinkedIn - https://t.co/EQ2RuxvRM0\\nGraduate Student at Texas A&M University. Mays Business School. Management Information Systems\", \"favourites_count\": 187, \"followers_count\": 13, \"friends_count\": 53, \"id\": 4276054402, \"id_str\": \"4276054402\", \"name\": \"Rohit\", \"profile_background_color\": \"C0DEED\", \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/4276054402/1448472147\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/669548138349395968/ZgpBe8kD_normal.jpg\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/669548138349395968/ZgpBe8kD_normal.jpg\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_border_color\": \"C0DEED\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"profile_use_background_image\": true, \"screen_name\": \"rohit_j27\", \"status\": {\"created_at\": \"Wed Jun 12 05:10:39 +0000 2019\", \"favorited\": true, \"id\": 1138674953845264384, \"id_str\": \"1138674953845264384\", \"lang\": \"en\", \"retweet_count\": 44090, \"retweeted\": true, \"retweeted_status\": {\"created_at\": \"Tue Jun 11 18:58:01 +0000 2019\", \"favorite_count\": 113835, \"favorited\": true, \"id\": 1138520780042465280, \"id_str\": \"1138520780042465280\", \"lang\": \"en\", \"retweet_count\": 44090, \"retweeted\": true, \"source\": \"<a href=\\\"http://twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web Client</a>\", \"text\": \"I\\u2019m always amazed by the disconnect between what we see in the news and the reality of the world around us. As my l\\u2026 https://t.co/F6OwugM850\", \"truncated\": true}, \"source\": \"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android</a>\", \"text\": \"RT @BillGates: I\\u2019m always amazed by the disconnect between what we see in the news and the reality of the world around us. As my late frien\\u2026\"}, \"statuses_count\": 49}\n"
     ]
    }
   ],
   "source": [
    "# test authentication\n",
    "print(twitter_api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For downloading tweets on a keyword (limit 180 every 15 mins)\n",
    "\n",
    "def buildTestSet(search_keyword):\n",
    "    try:\n",
    "        tweets_fetched = twitter_api.GetSearch(search_keyword, count = 100)\n",
    "        \n",
    "        print(\"Fetched \" + str(len(tweets_fetched)) + \" tweets for the term \" + search_keyword)\n",
    "        \n",
    "        return [{\"text\":status.text, \"label\":None} for status in tweets_fetched]\n",
    "    except:\n",
    "        print(\"Unfortunately, something went wrong..\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search keyword:USA\n",
      "Fetched 100 tweets for the term USA\n",
      "[{'text': 'My decision to appoint @usairforce General Charles Brown as the USA’s first-ever African American military service… https://t.co/XuyZT29O55', 'label': None}, {'text': 'We don’t have a policing problem in the USA, we have a parenting problem.', 'label': None}, {'text': 'The United States stands with the United Kingdom against the Chinese Communist Party’s coercive bullying tactics. W… https://t.co/KxPtWbiaPm', 'label': None}, {'text': 'RT @icretinoreal: O mundo ta violento assim pq\\nninguém usa mais a pulseirinha do reggae https://t.co/9oRJd4vyfh', 'label': None}]\n"
     ]
    }
   ],
   "source": [
    "search_term = input(\"Enter a search keyword:\")\n",
    "testDataSet = buildTestSet(search_term)\n",
    "\n",
    "print(testDataSet[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buidTrainingSet(corpusFile, tweetDataFile):\n",
    "    import csv\n",
    "    import time\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    with open(corpusFile,'r') as csvfile:\n",
    "        lineReader = csv.reader(csvfile,delimiter=',', quotechar=\"\\\"\")\n",
    "        for row in lineReader:\n",
    "            corpus.append({\"tweet_id\":row[2], \"label\":row[1], \"topic\":row[0]})\n",
    "            \n",
    "    rate_limit = 180\n",
    "    sleep_time = 900/180\n",
    "    \n",
    "    trainingDataSet = []\n",
    "    \n",
    "    for tweet in corpus:\n",
    "        try:\n",
    "            status = twitter_api.GetStatus(tweet[\"tweet_id\"])\n",
    "            print(\"Tweet fetched\" + status.text)\n",
    "            tweet[\"text\"] = status.text\n",
    "            trainingDataSet.append(tweet)\n",
    "            time.sleep(sleep_time) \n",
    "        except: \n",
    "            continue\n",
    "    # now we write them to the empty CSV file\n",
    "    with open(tweetDataFile,'w') as csvfile:\n",
    "        linewriter = csv.writer(csvfile,delimiter=',',quotechar=\"\\\"\")\n",
    "        for tweet in trainingDataSet:\n",
    "            try:\n",
    "                linewriter.writerow([tweet[\"tweet_id\"], tweet[\"text\"].encode('utf8') , tweet[\"label\"], tweet[\"topic\"]])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return trainingDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusFile = \"corpus.csv\"\n",
    "tweetDataFile = \"tweetDataFile.csv\"\n",
    "\n",
    "trainingData = buildTrainingSet(corpusFile, tweetDataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Training Dataset from File\n",
    "\n",
    "trainingData = []\n",
    "import csv  \n",
    "count = 0\n",
    "\n",
    "with open('tweetDataFile.csv','r') as csvfile:\n",
    "    lineReader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in lineReader:\n",
    "        if row != []:\n",
    "            trainingData.append({\"tweet_id\":row[0], \"text\":row[1], \"label\":row[2] ,\"topic\":row[3]})     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Tweets in The Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Rohit\n",
      "[nltk_data]     Jadhav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Rohit\n",
      "[nltk_data]     Jadhav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessTweets:\n",
    "    def __init__(self):\n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation) + ['AT_USER','URL'])\n",
    "        \n",
    "    def processTweets(self, list_of_tweets):\n",
    "        processedTweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            processedTweets.append((self._processTweet(tweet[\"text\"]),tweet[\"label\"]))\n",
    "        return processedTweets\n",
    "    \n",
    "    def _processTweet(self, tweet):\n",
    "        tweet = tweet.lower() # convert text to lower-case\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # remove URLs\n",
    "        tweet = re.sub('@[^\\s]+', 'AT_USER', tweet) # remove usernames \n",
    "        #tweet = re.sub('''[,b']''' ,'', tweet) # remove b' and b\" ***** Try to remove this noise*******\n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # remove the # in #hashtag\n",
    "        tweet = word_tokenize(tweet) # remove repeated characters (helloooooooo into hello) ***Wrong****\n",
    "        return [word for word in tweet if word not in self._stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetProcessor = PreProcessTweets()\n",
    "preprocessedTrainingSet = tweetProcessor.processTweets(trainingData)\n",
    "preprocessedTestSet = tweetProcessor.processTweets(testDataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching tweets against our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
